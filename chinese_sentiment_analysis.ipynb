{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Tensorflow进行中文自然语言处理--情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f('真好喝')=1$$\n",
    "$$f('太难喝了')=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**需要的库**  \n",
    "numpy  \n",
    "jieba  \n",
    "gensim  \n",
    "tensorflow  \n",
    "matplotlib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载必用的库\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 用来解压\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预训练词向量**  \n",
    "本教程使用了北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的\"chinese-word-vectors\" github链接为：  \n",
    "https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "如果你不知道word2vec是什么，我推荐以下一篇文章：  \n",
    "https://zhuanlan.zhihu.com/p/26306795  \n",
    "这里我们使用了\"chinese-word-vectors\"知乎Word + Ngram的词向量，可以从上面github链接下载，我们先加载预训练模型并进行一些简单测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请将下载的词向量压缩包放置在根目录 embeddings 文件夹里\n",
    "# 解压词向量, 有可能需要等待1-2分钟\n",
    "with open(\"embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词向量模型**  \n",
    "在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量，我们今天需要构建的LSTM神经网络模型并不能直接处理汉字文本，需要先进行分次并把词汇转换为词向量，步骤请参考下图，步骤的讲解会跟着代码一步一步来，如果你不知道RNN，GRU，LSTM是什么，我推荐deeplearning.ai的课程，网易公开课有免费中文字幕版，但我还是推荐有习题和练习代码部分的的coursera原版：  \n",
    "<img src='flowchart.jpg' style='width:400px;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.603470e-01,  3.677500e-01, -2.379650e-01,  5.301700e-02,\n",
       "       -3.628220e-01, -3.212010e-01, -1.903330e-01,  1.587220e-01,\n",
       "       -7.156200e-02, -4.625400e-02, -1.137860e-01,  3.515600e-01,\n",
       "       -6.408200e-02, -2.184840e-01,  3.286950e-01, -7.110330e-01,\n",
       "        1.620320e-01,  1.627490e-01,  5.528180e-01,  1.016860e-01,\n",
       "        1.060080e-01,  7.820700e-01, -7.537310e-01, -2.108400e-02,\n",
       "       -4.758250e-01, -1.130420e-01, -2.053000e-01,  6.624390e-01,\n",
       "        2.435850e-01,  9.171890e-01, -2.090610e-01, -5.290000e-02,\n",
       "       -7.969340e-01,  2.394940e-01, -9.028100e-02,  1.537360e-01,\n",
       "       -4.003980e-01, -2.456100e-02, -1.717860e-01,  2.037790e-01,\n",
       "       -4.344710e-01, -3.850430e-01, -9.366000e-02,  3.775310e-01,\n",
       "        2.659690e-01,  8.879800e-02,  2.493440e-01,  4.914900e-02,\n",
       "        5.996000e-03,  3.586430e-01, -1.044960e-01, -5.838460e-01,\n",
       "        3.093280e-01, -2.828090e-01, -8.563400e-02, -5.745400e-02,\n",
       "       -2.075230e-01,  2.845980e-01,  1.414760e-01,  1.678570e-01,\n",
       "        1.957560e-01,  7.782140e-01, -2.359000e-01, -6.833100e-02,\n",
       "        2.560170e-01, -6.906900e-02, -1.219620e-01,  2.683020e-01,\n",
       "        1.678810e-01,  2.068910e-01,  1.987520e-01,  6.720900e-02,\n",
       "       -3.975290e-01, -7.123140e-01,  5.613200e-02,  2.586000e-03,\n",
       "        5.616910e-01,  1.157000e-03, -4.341190e-01,  1.977480e-01,\n",
       "        2.519540e-01,  8.835000e-03, -3.554600e-01, -1.573500e-02,\n",
       "       -2.526010e-01,  9.355900e-02, -3.962500e-02, -1.628350e-01,\n",
       "        2.980950e-01,  1.647900e-01, -5.454270e-01,  3.888790e-01,\n",
       "        1.446840e-01, -7.239600e-02, -7.597800e-02, -7.803000e-03,\n",
       "        2.020520e-01, -4.424750e-01,  3.911580e-01,  2.115100e-01,\n",
       "        6.516760e-01,  5.668030e-01,  5.065500e-02, -1.259650e-01,\n",
       "       -3.720640e-01,  2.330470e-01,  6.659900e-02,  8.300600e-02,\n",
       "        2.540460e-01, -5.279760e-01, -3.843280e-01,  3.366460e-01,\n",
       "        2.336500e-01,  3.564750e-01, -4.884160e-01, -1.183910e-01,\n",
       "        1.365910e-01,  2.293420e-01, -6.151930e-01,  5.212050e-01,\n",
       "        3.412000e-01,  5.757940e-01,  2.354480e-01, -3.641530e-01,\n",
       "        7.373400e-02,  1.007380e-01, -3.211410e-01, -3.040480e-01,\n",
       "       -3.738440e-01, -2.515150e-01,  2.633890e-01,  3.995490e-01,\n",
       "        4.461880e-01,  1.641110e-01,  1.449590e-01, -4.191540e-01,\n",
       "        2.297840e-01,  6.710600e-02,  3.316430e-01, -6.026500e-02,\n",
       "       -5.130610e-01,  1.472570e-01,  2.414060e-01,  2.011000e-03,\n",
       "       -3.823410e-01, -1.356010e-01,  3.112300e-01,  9.177830e-01,\n",
       "       -4.511630e-01,  1.272190e-01, -9.431600e-02, -8.216000e-03,\n",
       "       -3.835440e-01,  2.589400e-02,  6.374980e-01,  4.931630e-01,\n",
       "       -1.865070e-01,  4.076900e-01, -1.841000e-03,  2.213160e-01,\n",
       "        2.253950e-01, -2.159220e-01, -7.611480e-01, -2.305920e-01,\n",
       "        1.296890e-01, -1.304100e-01, -4.742270e-01,  2.275500e-02,\n",
       "        4.255050e-01,  1.570280e-01,  2.975300e-02,  1.931830e-01,\n",
       "        1.304340e-01, -3.179800e-02,  1.516650e-01, -2.154310e-01,\n",
       "       -4.681410e-01,  1.007326e+00, -6.698940e-01, -1.555240e-01,\n",
       "        1.797170e-01,  2.848660e-01,  6.216130e-01,  1.549510e-01,\n",
       "        6.225000e-02, -2.227800e-02,  2.561270e-01, -1.006380e-01,\n",
       "        2.807900e-02,  4.597710e-01, -4.077750e-01, -1.777390e-01,\n",
       "        1.920500e-02, -4.829300e-02,  4.714700e-02, -3.715200e-01,\n",
       "       -2.995930e-01, -3.719710e-01,  4.622800e-02, -1.436460e-01,\n",
       "        2.532540e-01, -9.334000e-02, -4.957400e-02, -3.803850e-01,\n",
       "        5.970110e-01,  3.578450e-01, -6.826000e-02,  4.735200e-02,\n",
       "       -3.707590e-01, -8.621300e-02, -2.556480e-01, -5.950440e-01,\n",
       "       -4.757790e-01,  1.079320e-01,  9.858300e-02,  8.540300e-01,\n",
       "        3.518370e-01, -1.306360e-01, -1.541590e-01,  1.166775e+00,\n",
       "        2.048860e-01,  5.952340e-01,  1.158830e-01,  6.774400e-02,\n",
       "        6.793920e-01, -3.610700e-01,  1.697870e-01,  4.118530e-01,\n",
       "        4.731000e-03, -7.516530e-01, -9.833700e-02, -2.312220e-01,\n",
       "       -7.043300e-02,  1.576110e-01, -4.780500e-02, -7.344390e-01,\n",
       "       -2.834330e-01,  4.582690e-01,  3.957010e-01, -8.484300e-02,\n",
       "       -3.472550e-01,  1.291660e-01,  3.838960e-01, -3.287600e-02,\n",
       "       -2.802220e-01,  5.257030e-01, -3.609300e-02, -4.842220e-01,\n",
       "        3.690700e-02,  3.429560e-01,  2.902490e-01, -1.624650e-01,\n",
       "       -7.513700e-02,  2.669300e-01,  5.778230e-01, -3.074020e-01,\n",
       "       -2.183790e-01, -2.834050e-01,  1.350870e-01,  1.490070e-01,\n",
       "        1.438400e-02, -2.509040e-01, -3.376100e-01,  1.291880e-01,\n",
       "       -3.808700e-01, -4.420520e-01, -2.512300e-01, -1.328990e-01,\n",
       "       -1.211970e-01,  2.532660e-01,  2.757050e-01, -3.382040e-01,\n",
       "        1.178070e-01,  3.860190e-01,  5.277960e-01,  4.581920e-01,\n",
       "        1.502310e-01,  1.226320e-01,  2.768540e-01, -4.502080e-01,\n",
       "       -1.992670e-01,  1.689100e-02,  1.188860e-01,  3.502440e-01,\n",
       "       -4.064770e-01,  2.610280e-01, -1.934990e-01, -1.625660e-01,\n",
       "        2.498400e-02, -1.867150e-01, -1.954400e-02, -2.281900e-01,\n",
       "       -3.417670e-01, -5.222770e-01, -9.543200e-02, -3.500350e-01,\n",
       "        2.154600e-02,  2.318040e-01,  5.395310e-01, -4.223720e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['山东大学'].shape[0]\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "cn_model['山东大学']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity for Vector Space Models by Christian S. Perone\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66128117"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度\n",
    "cn_model.similarity('橘子', '橙子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6612812"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot（'橘子'/|'橘子'|， '橙子'/|'橙子'| ）\n",
    "np.dot(cn_model['橘子']/np.linalg.norm(cn_model['橘子']), \n",
    "cn_model['橙子']/np.linalg.norm(cn_model['橙子']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('高中', 0.724782407283783),\n",
       " ('本科', 0.6768534779548645),\n",
       " ('研究生', 0.6244412064552307),\n",
       " ('中学', 0.6088204383850098),\n",
       " ('大学本科', 0.5959085822105408),\n",
       " ('初中', 0.5883589386940002),\n",
       " ('读研', 0.5778335928916931),\n",
       " ('职高', 0.5767995715141296),\n",
       " ('大学毕业', 0.5767451524734497),\n",
       " ('师范大学', 0.5708828568458557)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "cn_model.most_similar(positive=['大学'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 老师 会计师 程序员 律师 医生 老人 中:\n",
      "不是同一类别的词为: 老人\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '老师 会计师 程序员 律师 医生 老人'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('出轨', 0.6100173592567444)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','劈腿'], negative=['男人'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练语料**  \n",
    "本教程使用了谭松波老师的酒店评论语料，即使是这个语料也很难找到下载链接，在某博客还得花积分下载，而我不知道怎么赚取积分，后来好不容易找到一个链接但竟然是失效的，再后来尝试把链接粘贴到迅雷上终于下载了下来，希望大家以后多多分享资源。  \n",
    "训练样本分别被放置在两个文件夹里：\n",
    "分别的pos和neg，每个文件夹里有2000个txt文件，每个文件内有一段评语，共有4000个训练样本，这样大小的样本数据在NLP中属于非常迷你的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "# 分别为 正面评价'pos'文件夹 和 负面评价'neg'文件夹\n",
    "# 每个文件夹中有2000个txt文件，每个文件中是一例评价\n",
    "import os\n",
    "pos_txts = os.listdir('pos')\n",
    "neg_txts = os.listdir('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 4000\n"
     ]
    }
   ],
   "source": [
    "print( '样本总共: '+ str(len(pos_txts) + len(neg_txts)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "# 这里和视频课程不一样, 因为很多同学反应按照视频课程里的读取方法会乱码,\n",
    "# 经过检查发现是原始文本里的编码是gbk造成的,\n",
    "# 这里我进行了简单的预处理, 以避免乱码\n",
    "train_texts_orig = []\n",
    "# 文本所对应的labels, 也就是标记\n",
    "train_target = []\n",
    "with open(\"positive_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])\n",
    "\n",
    "with open(\"negative_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "# from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.python.keras.optimizers import RMSprop\n",
    "# from tensorflow.python.keras.optimizers import Adam\n",
    "# from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词和tokenize**  \n",
    "首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            # cut_list[i] = cn_model.vocab[word].index\n",
    "            cut_list[i] = cn_model.key_to_index[word]\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引长度标准化**  \n",
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.42575"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向tokenize**  \n",
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            # text = text + cn_model.index2word[i]\n",
    "            text = text + cn_model.index_to_key[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下可见，训练样本的极性并不是那么精准，比如说下面的样本，对早餐并不满意，但被定义为正面评价，这会迷惑我们的模型，不过我们暂时不对训练样本进行任何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备Embedding Matrix**  \n",
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。  \n",
    "注意我们只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前20000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    # embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index_to_key[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index_to_key[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding（填充）和truncating（修剪）**  \n",
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         290,  3053,    57,   169,    73,     1,    25, 11216,    49,\n",
       "         163, 15985,     0,     0,    30,     8,     0,     1,   228,\n",
       "         223,    40,    35,   653,     0,     5,  1642,    29, 11216,\n",
       "        2751,   500,    98,    30,  3159,  2225,  2146,   371,  6285,\n",
       "         169, 27396,     1,  1191,  5432,  1080, 20055,    57,   562,\n",
       "           1, 22671,    40,    35,   169,  2567,     0, 42665,  7761,\n",
       "         110,     0,     0, 41281,     0,   110,     0, 35891,   110,\n",
       "           0, 28781,    57,   169,  1419,     1, 11670,     0, 19470,\n",
       "           1,     0,     0,   169, 35071,    40,   562,    35, 12398,\n",
       "         657,  4857])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前2000样本为1，后2000为0\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                        房间很大还有海景阳台走出酒店就是沙滩非常不错唯一遗憾的就是不能刷 不方便\n",
      "class:  1\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[35]))\n",
    "print('class: ',y_train[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。   \n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$\n",
    "输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "236\n"
     ]
    }
   ],
   "source": [
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "\n",
    "print(embedding_dim)\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在2019年6月10日修改了一些大坑的bug, 可能是数据的顺序变了, \n",
    "# 结果模型训练的效果没有去年最早的时候效果好了, \n",
    "# 有兴趣的同学可以调整一下模型参数, 看看会不会有更好的结果\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**  \n",
    "我在这个教程中尝试了几种神经网络结构，因为训练样本比较少，所以我们可以尽情尝试，训练过程等待时间并不长：  \n",
    "**GRU：**如果使用GRU的话，测试样本可以达到87%的准确率，但我测试自己的文本内容时发现，GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。  \n",
    "**BiLSTM：**测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆，有兴趣的朋友可以深入研究一下。  \n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU的代码\n",
    "# model.add(GRU(units=32, return_sequences=True))\n",
    "# model.add(GRU(units=16, return_sequences=True))\n",
    "# model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "# optimizer = Adam(lr=1e-3)\n",
    "optimizer = Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 236, 300)          15000000  \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 236, 128)          186880    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 16)                9280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15196181 (57.97 MB)\n",
      "Trainable params: 196181 (766.33 KB)\n",
      "Non-trainable params: 15000000 (57.22 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid file format!\n"
     ]
    }
   ],
   "source": [
    "# import h5py\n",
    "\n",
    "# file_path = \"sentiment_checkpoint.h5\"\n",
    "\n",
    "# if h5py.is_hdf5(file_path):\n",
    "#     # 文件格式正确，进行打开操作\n",
    "#     f = h5py.File(file_path, \"r\")\n",
    "# else:\n",
    "#     print(\"Invalid file format!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open file (file signature not found)\n"
     ]
    }
   ],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# model.load_weights(path_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论**  \n",
    "我们首先对测试样本进行预测，得到了还算满意的准确度。  \n",
    "之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 62ms/step - loss: 0.6781 - accuracy: 0.5100\n",
      "Accuracy:51.00%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "# print(\"Test loss:\", result[0])\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # cut_list[i] = cn_model.vocab[word].index\n",
    "            cut_list[i] = cn_model.key_to_index[word]\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "是一例负面评价 output=0.46\n",
      "酒店卫生条件非常不好\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "是一例负面评价 output=0.45\n",
      "床铺非常舒适\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "是一例负面评价 output=0.47\n",
      "房间很凉，不给开暖气\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "是一例负面评价 output=0.45\n",
      "房间很凉爽，空调冷气很足\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "是一例负面评价 output=0.46\n",
      "酒店环境不好，住宿体验很不好\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "是一例负面评价 output=0.45\n",
      "房间隔音不到位\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "是一例负面评价 output=0.45\n",
      "晚上回来发现没有打扫卫生\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "是一例负面评价 output=0.45\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "是一例负面评价 output=0.45\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**错误分类的文本**\n",
    "经过查看，发现错误分类的文本的含义大多比较含糊，就算人类也不容易判断极性，如index为101的这个句子，好像没有一点满意的成分，但这例子评价在训练样本中被标记成为了正面评价，而我们的模型做出的负面评价的预测似乎是合理的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                        由于2007年 有一些新问题可能还没来得及解决我因为工作需要经常要住那里所以慎重的提出以下 ：1 后 的 淋浴喷头的位置都太高我换了房间还是一样很不好用2 后的一些管理和服务还很不到位尤其是前台入住和 时代效率太低每次 都超过10分钟好像不符合 宾馆的要求\n",
      "预测的分类 0\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx=101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                 还是很 设施也不错但是 和以前 比急剧下滑了 和客房 的服务极差幸好我不是很在乎\n",
      "预测的分类 0\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
